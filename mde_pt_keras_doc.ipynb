{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/skj092/monocular-depth-estimation/blob/main/mde_pt_keras_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iAqcWuXXKpm",
    "outputId": "5536815a-8881-40fe-f642-55d8587bfb97"
   },
   "outputs": [],
   "source": [
    "# !pip install -qq pytorch_msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/mde/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from loss_fns import calculate_loss, abs_rel, log10_mae, log10_rmse, threshold_accuracy, delta1, delta2, delta3, CombinedDepthLoss\n",
    "from model2 import DepthEstimationModel\n",
    "# from fastai.vision.all import *\n",
    "import keras\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from engine import train_epoch, validate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Download\n",
    "# annotation_folder = \"/dataset/\"\n",
    "# if not os.path.exists(os.path.abspath(\".\") + annotation_folder):\n",
    "#     annotation_zip = keras.utils.get_file(\n",
    "#         \"val.tar.gz\",\n",
    "#         cache_subdir=os.path.abspath(\".\"),\n",
    "#         origin=\"http://diode-dataset.s3.amazonaws.com/val.tar.gz\",\n",
    "#         extract=True,\n",
    "#     )\n",
    "\n",
    "# wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "# wandb.init(project=\"DepthEstimation\", name=\"FastAI_Training\",)\n",
    "\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading and preprocessing\n",
    "train_path = \"val_extracted/val/indoors\"\n",
    "filelist = []\n",
    "for root, dirs, files in os.walk(train_path):\n",
    "    for file in files:\n",
    "        filelist.append(os.path.join(root, file))\n",
    "\n",
    "filelist.sort()\n",
    "\n",
    "images = sorted([x for x in filelist if x.endswith(\".png\")])\n",
    "depths = sorted([x for x in filelist if x.endswith(\"_depth.npy\")])\n",
    "masks = sorted([x for x in filelist if x.endswith(\"_depth_mask.npy\")])\n",
    "\n",
    "# Get base names\n",
    "depth_bases = {os.path.basename(x).replace(\"_depth.npy\", \"\") for x in depths}\n",
    "mask_bases = {os.path.basename(x).replace(\"_depth_mask.npy\", \"\") for x in masks}\n",
    "valid_bases = depth_bases & mask_bases\n",
    "\n",
    "# Filter valid files only\n",
    "valid_images = [x for x in images if os.path.basename(x).replace(\".png\", \"\") in valid_bases]\n",
    "valid_depths = [x for x in depths if os.path.basename(x).replace(\"_depth.npy\", \"\") in valid_bases]\n",
    "valid_masks = [x for x in masks if os.path.basename(x).replace(\"_depth_mask.npy\", \"\") in valid_bases]\n",
    "\n",
    "# Build DataFrame\n",
    "data = {\n",
    "    \"image\": valid_images,\n",
    "    \"depth\": valid_depths,\n",
    "    \"mask\": valid_masks,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Total train samples: {len(df)}\")\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader setup\n",
    "def open_image(fn):\n",
    "    img = cv2.imread(fn)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    return torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "def load_depth_masked(row):\n",
    "    depth = np.load(row['depth']).squeeze()\n",
    "    mask = np.load(row['mask']) > 0\n",
    "    epsilon = 1e-6\n",
    "    max_depth = min(300, np.percentile(depth[mask], 99))\n",
    "    depth = np.clip(depth, epsilon, max_depth)\n",
    "    depth_log = np.zeros_like(depth)\n",
    "    depth_log[mask] = np.log(depth[mask])\n",
    "    depth_log = np.clip(depth_log, np.log(epsilon), np.log(max_depth))\n",
    "    depth_log = cv2.resize(depth_log, (256, 256))\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        depth_norm = (depth_log / np.log(max_depth))\n",
    "        depth_norm = np.nan_to_num(depth_norm, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    depth_uint8 = (depth_norm * 255).astype(np.uint8)\n",
    "    return torch.tensor(depth_uint8, dtype=torch.float32).unsqueeze(0) / 255.0  # Add channel dimension and normalize\n",
    "\n",
    "def get_x(row): return open_image(row['image'])\n",
    "def get_y(row): return load_depth_masked(row)\n",
    "\n",
    "class DepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        x = get_x(row)\n",
    "        y = get_y(row)\n",
    "        return x, y\n",
    "\n",
    "train_ds = DepthDataset(train_df)\n",
    "valid_ds = DepthDataset(valid_df)\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a batch of data\n",
    "def visualize_batch(dl):\n",
    "    for x, y in dl:\n",
    "        x = x.permute(0, 2, 3, 1).numpy()  # Convert to [B, H, W, C] format\n",
    "        y = y.squeeze().numpy()  # Remove channel dimension\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=len(x), figsize=(15, 5))\n",
    "        for i in range(len(x)):\n",
    "            axes[0, i].imshow(x[i])\n",
    "            axes[0, i].set_title('Input Image')\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(y[i], cmap='gray')\n",
    "            axes[1, i].set_title('Depth Map')\n",
    "            axes[1, i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break  # Show only one batch\n",
    "\n",
    "visualize_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7NswFknWlQL",
    "outputId": "d6a26f2c-6686-4408-dfae-132f24a22362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples: 325\n",
      "Training samples: 260, Validation samples: 65\n"
     ]
    }
   ],
   "source": [
    "loss_func = CombinedDepthLoss(ssim_loss_weight=1.0, l1_loss_weight=1.0, edge_loss_weight=1.0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DepthEstimationModel(in_channels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_dl), epochs=10)\n",
    "\n",
    "def train_model(model, train_dl, valid_dl, optimizer, loss_func, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dl, optimizer, loss_func, device)\n",
    "        valid_loss, metrics = validate_epoch(model, valid_dl, loss_func, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        # scheduler.step()\n",
    "        # Log to Weights & Biases\n",
    "        # wandb.log({\n",
    "        #     'epoch': epoch + 1,\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'valid_loss': valid_loss,\n",
    "        #     **metrics\n",
    "        # })\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dt1TSmAZi5v",
    "outputId": "b1d8a187-fe53-4b14-caf9-40073befbc9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:29<00:00,  1.72s/it]\n",
      "  0%|                                                                                                                                                                   | 0/5 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'abs_rel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m      3\u001b[39m torch.save(model.state_dict(), \u001b[33m'\u001b[39m\u001b[33mdepth_estimation_model.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_dl, valid_dl, optimizer, loss_func, device, epochs)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    167\u001b[39m     train_loss = train_epoch(model, train_dl, optimizer, loss_func, device)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     valid_loss, metrics = \u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Valid Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codes/internal-projects/knowledge-distillation/src/mde/engine.py:38\u001b[39m, in \u001b[36mvalidate_epoch\u001b[39m\u001b[34m(model, valid_dl, loss_func, device)\u001b[39m\n\u001b[32m     36\u001b[39m loss = loss_func(pred, y)\n\u001b[32m     37\u001b[39m total_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m metrics[\u001b[33m'\u001b[39m\u001b[33mabs_rel\u001b[39m\u001b[33m'\u001b[39m] += \u001b[43mabs_rel\u001b[49m(pred, y).item()\n\u001b[32m     39\u001b[39m metrics[\u001b[33m'\u001b[39m\u001b[33mlog10_mae\u001b[39m\u001b[33m'\u001b[39m] += log10_mae(pred, y).item()\n\u001b[32m     40\u001b[39m metrics[\u001b[33m'\u001b[39m\u001b[33mlog10_rmse\u001b[39m\u001b[33m'\u001b[39m] += log10_rmse(pred, y).item()\n",
      "\u001b[31mNameError\u001b[39m: name 'abs_rel' is not defined"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dl, valid_dl, optimizer, loss_func, device, epochs=10)\n",
    "torch.save(model.state_dict(), 'depth_estimation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUCMWX-sW6Op"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, valid_dl, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(valid_dl):\n",
    "            x = x.to(device)\n",
    "            pred = model(x)\n",
    "            pred = F.interpolate(pred, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Get only the first image from the batch\n",
    "            pred_img = pred[0].cpu().squeeze().numpy()\n",
    "            y_img = y[0].cpu().squeeze().numpy()\n",
    "\n",
    "            # Convert to uint8 for visualization\n",
    "            pred_vis = (pred_img * 255).astype(np.uint8)\n",
    "            y_vis = (y_img * 255).astype(np.uint8)\n",
    "\n",
    "            # Plot side-by-side\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axs[0].imshow(pred_vis, cmap='gray')\n",
    "            axs[0].set_title(f'Predicted Depth {i}')\n",
    "            axs[0].axis('off')\n",
    "\n",
    "            axs[1].imshow(y_vis, cmap='gray')\n",
    "            axs[1].set_title(f'Ground Truth Depth {i}')\n",
    "            axs[1].axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            if i >= 4:  # Show only 5 batches\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu2n-GemY8S-"
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilDtnsfsbwE5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMu0vgbEOOcCkNSZBaTz6SY",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
